% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Threats to Validity}\label{chapter:threats}
This paper acknowledges a few threats to validity. First, as it was mentioned earlier in the paper, the buggy-to-inserted line ratio is not the ultimate way to judge the quality of developers. The technique does not take into account the soft skills like requirement analysis, communication, time management, or the code characteristics like loose coupling, proper documentation, performance, simplicity, and readability. What is more, the method can only find the introduced bugs that were reported and fixed. If a defect was discovered and fixed immediately without being reported, it would not be taken into account for the analysis. Similarly, if a defect was discovered, but the fix required adding additional logic rather than modifying existing code, the source of such a bug could not be found using the SZZ algorithm\parencite{Sliwerski}, and no developer would be blamed for it. Only 10\% of fixes inserted new lines instead of modifying existing ones, so the effect on the results should not be significant. The method used in this paper also relies heavily on proper management of repositories. Any bugs not reported, issues fixed but not closed, or fix messages not following the correct syntax standards may lower the precision of the measurements. To mitigate this concern and increase the accuracy of fix-finding method, the repositories used for research were  carefully selected, heuristics were extensively tested, and results were verified. \par

Merge commits pose another threat to validity. Sometimes merging a fix branch to a master branch, creates a merge commit. This happens when the master is not the direct ancestor of the fix branch. Merge commits contain the same file modifications as the parents; therefore, including merge commits in the analysis can result in noting duplicate changes from merged files. For that reason, I excluded merge commits from the analysis. This way no data was lost but the duplicate modifications were avoided. \par

Similarly, test commits pose a threat to accuracy of the fix-finding method. As manually verified, some of the repositories adopt the convention to add tests for a fixed defect by directly referencing the bug in the commit message. This paper revised a special method to lower the syntactic score for those commits. The suspected test commits were not excluded from analysis the way merge commits were because on occasion some commits could include the test keywords while indeed being fixes, rather than tests. By setting the syntactic score to 0 for suspected test cases, true fixes with strong semantic score were still recognized, while the tests were rejected. The full set of rules recognizing fixes was verified by selecting random samples of fix positives and fix negatives from each repository and manually inspecting the results. \par

The simple method of merging authors who contributed to multiple repositories could pose additional threat to validity. This paper used the heuristics that merge only developers with the same email address. Therefore, a single author with two different email addresses would be considered as two different authors. The method was chosen because it provides higher precision–no false positives are possible, while recall is not believed to be significantly impacted. \par

Cloned code in selected repositories is another recognized limitation. Not all of the repositories were verified for cloned code. Because Dejavu, the cloned code verification tool, does not contain data on all the repositories, not all of them could be verified. If a project contains significant proportion of code that was copied from other repositories, it could misrepresent developers’ quality and their focus. To mitigate this concern, the repositories were carefully selected for this project. The repositories that were verified either did not include cloned code, or were only cloned from. The repositories that were not verified for cloned code were well known, which is believed to lower the possibility of contributors cloning code from other projects.  \par

Finally, measuring focus based solely on the diversity of file extensions may not be the optimal method. If a developer worked on six files with .php extension, the method used in the paper would classify her as focused. Comparatively, a developer who worked on six extensions, including two .html, one .js and three .php, was described as not focused. There was no further examination of the modified files. It could be possible that the .php files contained html code and only that part of the files was modified. In that case, the developer was more focused than originally estimated, since five out of six modified files in fact used html technology. Additionally, temporal properties of switching between file extensions could be added to create a more fine-grained measure of focus. This idea is beyond the scope of this research and it is described in more details in the Future Research section. \par

With the last limitation in mind, this project used extension-based measure as the approximation of focus. One may assume that a developer who worked on one technology during a day was more focused than a developer who worked on five different technologies. Even if the touched files were related, adjusting the mindset to different coding styles lowers developer’s focus. It is hard, however, to measure the level of focus for a year because developers work on many technologies during such long period of time. In this research, focus was measured over different, often long, time periods. This is why the main findings are presented in the form of a high-level comparison of focus of two large groups of developers, rather than a fine-grained explanation of each developer’s focus. Even when measuring focus over a year or four years, entropy provides accurate estimation to compare two developers groups at a high level. \par


